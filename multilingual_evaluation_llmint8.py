# -*- coding: utf-8 -*-
"""LLM_int8

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/fifrio/llm-int8.1dcd2e00-57b4-4b8b-8741-b14fac23640b.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251008/auto/storage/goog4_request%26X-Goog-Date%3D20251008T122549Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2eb709c689805c49834502c067128844671bc63e5c5ba2c19e579a01fb43e20d42c4febee1eaefc80500c24df36a75e32c780ea0317ff0417c6436983d57831ea22d7b9afaae8f032d0d278f133561a3b4f65769642ade60e869ab4486f17664dc886abacfbdf3facf01b3a274aa6b0f38e73f43b1961043bce572da2179bc7ab93701a54630823051a89cbaeada59d4799e8b7f7736bf23a57b98b004ab7f246a2cafb27cbb31f7fa84b47eb661f24d9a258e471a23642c0c4eb51b8303723d55076d386ce925e6f1994b151d6993a40bf962768897a6d11c0ac6e7b5d17a3668ff301e4a04c10a631df3e611514d36de01f2fa5301e8686c0def60d2f08d19
"""

import os
from dotenv import load_dotenv

import torch
from lm_eval.models.huggingface import HFLM
from lm_eval import simple_evaluate

import numpy as np
import pandas as pd
import pickle
import pprint

import bitsandbytes
from transformers import BitsAndBytesConfig
import wandb

from pytablewriter import LatexTableWriter, MarkdownTableWriter

from transformers import AutoTokenizer, AutoModelForCausalLM

# Argument
import argparse

parser = argparse.ArgumentParser("args_gptq")
parser.add_argument("--bit", type=int)
args = parser.parse_args()
bit = args.bit

"""# Parameter"""

load_dotenv()

if "HF_KEY" not in os.environ:
    raise EnvironmentError("HF_KEY environment variable is not defined. Please set it before running the application.")
if "WANDB_KEY" not in os.environ:
    raise EnvironmentError("WANDB_KEY environment variable is not defined. Please set it before running the application.")

hf_key = os.environ["HF_KEY"]
wandb_key = os.environ["WANDB_KEY"]

# WandB
wandb.login(key=wandb_key)

ENTITY = wandb.apis.PublicApi().default_entity
PROJECT = "calibration-on-quantized-multilingual"

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device_str = 'cuda' if torch.cuda.is_available() else 'cpu'
batch_size = 4
bit_lst = [4, 8]

# Model
model_id = "google/gemma-3-1b-pt"
lang = "Uncalibrated"

# Evaluation
evaluation_dataset = "xnli"
num_shot = 3
apply_chat_template = False

# Quantization Config
quantization_technique = "bnb"
calibration_language = "Uncalibrated"
granularity = "vector"
group_size = None
num_calibration_samples = None
max_sequence_length = None
symmetry = False

output_path_bnb = f"./{model_id.split("/")[-1]}_{quantization_technique}_{bit}bit_{lang}"
output_result_bnb = f"./{evaluation_dataset}_{num_shot}shot_{quantization_technique}_{bit}bit_{lang}.json"

# WandB Logging
# output_huggingface_gptq = f"fifrio/gemma-3-1b-pt-bnb-{bit}bit-calibration-{lang}"
output_huggingface_gptq = None
wandb_config = {
    'base_model': model_id,
    'quantization_technique': quantization_technique,
    'calibration_language': lang,
    'bit_width': f"{bit}-bit",
    "granularity": granularity,
    "group_size": group_size,
    'num_calibration_samples': num_calibration_samples,
    'max_sequence_length': max_sequence_length,
    'symmetry': symmetry,
    'output_huggingface': output_huggingface_gptq,
    'evaluation_dataset': evaluation_dataset,
    'num_shot': num_shot,
    'apply_chat_template': apply_chat_template,
}
wandb_runname = f"{model_id.split('/')[-1]}-{quantization_technique}-{bit}bit-{lang}-{evaluation_dataset}"

"""# Function"""

def lm_eval_wrapper(model, tokenizer, device: str):
  return HFLM(
    pretrained = model,
    backend = "causal",
    tokenizer = tokenizer,
    trust_remote_code = True,
    device = device,
    dtype = torch.float16,
    batch_size=batch_size,
)

def eval_model(model, device='cpu'):
  return simple_evaluate(
      model=model,
      tasks=[evaluation_dataset],
            # "xwinograd",
            #  "xstorycloze"],
      device=device,
      num_fewshot=num_shot,
      apply_chat_template = apply_chat_template,
    #   gen_kwargs={'temperature': 0},
      predict_only=False,
      log_samples=True,
      batch_size=batch_size,
      random_seed=1234,
  )

"""## Calibration data -> BNB dont use Calibration

> https://huggingface.co/datasets/cambridgeltl/xcopa
>
> https://huggingface.co/datasets/Muennighoff/xwinograd
>
> https://huggingface.co/datasets/juletxara/xstory_cloze

## BitsandBytes
"""

import time
print(f"Start Evaluating")
start_time = time.time()

if bit == 4:
    cfg = BitsAndBytesConfig(load_in_8bit=True, bnb_8bit_compute_dtype=torch.bfloat16, token=hf_key)
    tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_key)
elif bit == 8:
    cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, token=hf_key)
    tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_key)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=cfg, device_map=device, token=hf_key)

model = lm_eval_wrapper(model, tokenizer, device=device_str)

result = eval_model(model, device=device_str)

print(f"Finish Evaluating")
print(f"Time span: {time.time()}-{start_time}")

import json
with open(output_result_bnb, 'wb') as file:
    json.dump(result, file)

print(result)

# Script from the lm_eval library
import json
import logging
import os

from pytablewriter import LatexTableWriter, MarkdownTableWriter


logger = logging.getLogger(__name__)


def make_table(result_dict):
    """Generate table of results."""
    md_writer = MarkdownTableWriter()
    latex_writer = LatexTableWriter()
    md_writer.headers = ["Task", "Version", "Metric", "Value", "", "Stderr"]
    latex_writer.headers = ["Task", "Version", "Metric", "Value", "", "Stderr"]

    values = []

    for k, dic in sorted(result_dict["results"].items()):
        version = result_dict["versions"][k]
        percent = k == "squad2"
        for m, v in dic.items():
            if m.endswith("_stderr"):
                continue
            if m == 'alias':
                continue

            if m + "_stderr" in dic:
                se = dic[m + "_stderr"]
                if percent or m == "ppl":
                    values.append([k, version, m, "%.2f" % v, "±", "%.2f" % se])
                else:
                    values.append(
                        [k, version, m, "%.2f" % (v * 100), "±", "%.2f" % (se * 100)]
                    )
            else:
                if percent or m == "ppl":
                    values.append([k, version, m, "%.2f" % v, "", ""])
                else:
                    try:
                      values.append([k, version, m, "%.2f" % (v * 100), "", ""])
                    except:
                      pass
            k = ""
            version = ""
    md_writer.value_matrix = values
    latex_writer.value_matrix = values

    # todo: make latex table look good
    # print(latex_writer.dumps())

    return md_writer.dumps()

print(make_table(result))

def make_table(file_json):
  # Script from the lm_eval library
    """Generate table of results."""
    md_writer = MarkdownTableWriter()
    latex_writer = LatexTableWriter()
    md_writer.headers = ["Task", "Language", "Version", "Metric", "Value", "", "Stderr"]
    latex_writer.headers = ["Task", "Language", "Version", "Metric", "Value", "", "Stderr"]

    values = []
    # XCopa, XWinograd, XStoryCloze
    # tasks_values = [[], [], []]
    tasks_values = [[]]

    for k, dic in sorted(file_json["results"].items()):
        if "_" not in k:
            continue

        version = file_json["versions"][k]
        percent = k == "squad2"

        task = k.split("_")[0]
        lang = k.split("_")[1]
        if task == "xnli":
            task_index = 0
        elif task == "xwinograd":
            task_index = 1
        elif task == "xstorycloze":
            task_index = 2
        for m, v in dic.items():
            # print(m, dic)
            if m.endswith("_stderr,none"):
                continue
            if m == 'alias':
                continue

            m = m.split(",")[0]

            # The eval provide std error, for now, skip those
            # if m + "_stderr,none" in dic:
                # se = dic[m + "_stderr,none"]
                # if percent or m == "ppl":
                #     values.append([task, lang, version, m, "%.2f" % v, "±", "%.2f" % se])
                # else:
                #     values.append(
                #         [task, lang, version, m, "%.2f" % (v * 100), "±", "%.2f" % (se * 100)]
                #     )
            # else:
            if percent or m == "ppl":
                values.append([task, lang, version, m, "%.2f" % v, "", ""])
                tasks_values[task_index].append([task, lang, version, v])
            else:
                try:
                  values.append([task, lang, version, m, "%.2f" % (v * 100), "", ""])
                  tasks_values[task_index].append([task, lang, version, v])
                except:
                  pass
            k = ""
            version = ""
    md_writer.value_matrix = values
    latex_writer.value_matrix = values

    # todo: make latex table look good
    # print(latex_writer.dumps())

    return tasks_values

"""# WandB Logging"""

import pprint
pprint.pformat(result)

with wandb.init(
    entity=ENTITY, project=PROJECT, config=wandb_config, name=wandb_runname
) as run:
    # Log Accuracy
    clean_result = make_table(result)[0]
    for task_name, lang_eval, task_version, accuracy in clean_result:
        run.summary[f"{task_name}_acc_{lang_eval}"] = accuracy

    # Log Result
    # columns = ["Eval Dataset", "Result"]
    # data = [["xnli", pprint.pformat(result)]]
    # table = wandb.Table(data=data, columns=columns)
    # run.log({"result": table})

    artifact = wandb.Artifact(name=f"{wandb_runname}-result", type="eval-result")
    artifact.add_file(local_path=f"./{output_result_bnb}", name=f"{evaluation_dataset}-result.json")
    artifact.save()

