# -*- coding: utf-8 -*-
"""Multilingual Calibration - Slim LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xJaKsoEojxOt5LwojaVWiCXXVi0ZRLRa
"""

"""# Usual Coding"""
import os
from dotenv import load_dotenv

import torch
from lm_eval.models.huggingface import HFLM
from lm_eval import simple_evaluate

import numpy as np
import pandas as pd
import pickle
import pprint

from transformers import AutoTokenizer, AutoModelForCausalLM
import wandb

from pytablewriter import LatexTableWriter, MarkdownTableWriter

from huggingface_hub import create_repo
from huggingface_hub import HfApi

# Argument
import argparse

parser = argparse.ArgumentParser("args_slim-llm")
parser.add_argument("--lang", type=str)
parser.add_argument("--bit", type=int)
args = parser.parse_args()
lang = args.lang
bit = args.bit

# Login

load_dotenv()

if "HF_KEY" not in os.environ:
    raise EnvironmentError("HF_KEY environment variable is not defined. Please set it before running the application.")
if "WANDB_KEY" not in os.environ:
    raise EnvironmentError("WANDB_KEY environment variable is not defined. Please set it before running the application.")

hf_key = os.environ["HF_KEY"]
wandb_key = os.environ["WANDB_KEY"]

from huggingface_hub import login
login(token=hf_key)

# WandB
wandb.login(key=wandb_key)

ENTITY = wandb.apis.PublicApi().default_entity
PROJECT = "calibration-on-quantized-multilingual"

"""# Parameter"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device_str = 'cuda' if torch.cuda.is_available() else 'cpu'
batch_size = 4

# Model
model_id = "google/gemma-3-1b-pt"

# Evaluation
evaluation_dataset = "xnli"
num_shot = 3
apply_chat_template = False

# Quantization Config
quantization_technique = "slimllm"
granularity = "group"
group_size = 128
num_calibration_samples = 512
max_sequence_length = 2048
symmetry = False

model_path_slimllm = f"./slim-llm/output/{model_id.replace("/", "_")}_flores_{lang}_{bit}bit_{group_size}"
output_result_slimllm = f"./{evaluation_dataset}_{num_shot}shot_{quantization_technique}_{bit}bit_{lang}.json"

output_huggingface_gptq = f"fifrio/{model_id.split("/")[-1]}-{quantization_technique}_{bit}bit_{lang}"
wandb_config = {
    'base_model': model_id,
    'quantization_technique': quantization_technique,
    'calibration_language': lang,
    'bit_width': f"{bit}-bit",
    "granularity": granularity,
    "group_size": group_size,
    'num_calibration_samples': num_calibration_samples,
    'max_sequence_length': max_sequence_length,
    'symmetry': symmetry,
    'output_huggingface': output_huggingface_gptq,
    'evaluation_dataset': evaluation_dataset,
    'num_shot': num_shot,
    'apply_chat_template': apply_chat_template,
}
wandb_runname = f"{model_id.split('/')[-1]}-{quantization_technique}-{bit}bit-{lang}-{evaluation_dataset}"

"""# Function"""

def lm_eval_wrapper(model, tokenizer, device: str):
  return HFLM(
    pretrained = model,
    backend = "causal",
    tokenizer = tokenizer,
    trust_remote_code = True,
    device = device,
    dtype = torch.float16,
    batch_size=batch_size,
)

def eval_model(model, device='cpu'):
  return simple_evaluate(
      model=model,
      tasks=[evaluation_dataset],
      device=device,
      num_fewshot=num_shot,
      apply_chat_template = apply_chat_template,
    #   gen_kwargs={'temperature': 0},
      predict_only=False,
      log_samples=True,
      batch_size=batch_size,
      random_seed=1234,
  )

"""# Parameter"""

import time
print(f"Start Evaluating")
start_time = time.time()

"""# Load Model"""
# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-pt")
model = AutoModelForCausalLM.from_pretrained(model_path_slimllm, device_map=device, device=device)

model = lm_eval_wrapper(model, tokenizer, device_str)

result = eval_model(model, device_str)

import json
with open(output_result_slimllm, 'wb') as file:
    json.dump(result, file)

print(result)

# Script from the lm_eval library
import json
import logging
import os

from pytablewriter import LatexTableWriter, MarkdownTableWriter


logger = logging.getLogger(__name__)


def make_table(result_dict):
    """Generate table of results."""
    md_writer = MarkdownTableWriter()
    latex_writer = LatexTableWriter()
    md_writer.headers = ["Task", "Version", "Metric", "Value", "", "Stderr"]
    latex_writer.headers = ["Task", "Version", "Metric", "Value", "", "Stderr"]

    values = []

    for k, dic in sorted(result_dict["results"].items()):
        version = result_dict["versions"][k]
        percent = k == "squad2"
        for m, v in dic.items():
            if m.endswith("_stderr"):
                continue
            if m == 'alias':
                continue

            if m + "_stderr" in dic:
                se = dic[m + "_stderr"]
                if percent or m == "ppl":
                    values.append([k, version, m, "%.2f" % v, "±", "%.2f" % se])
                else:
                    values.append(
                        [k, version, m, "%.2f" % (v * 100), "±", "%.2f" % (se * 100)]
                    )
            else:
                if percent or m == "ppl":
                    values.append([k, version, m, "%.2f" % v, "", ""])
                else:
                    try:
                      values.append([k, version, m, "%.2f" % (v * 100), "", ""])
                    except:
                      pass
            k = ""
            version = ""
    md_writer.value_matrix = values
    latex_writer.value_matrix = values

    # todo: make latex table look good
    # print(latex_writer.dumps())

    return md_writer.dumps()

print(make_table(result))


def make_table(file_json):
  # Script from the lm_eval library
    """Generate table of results."""
    md_writer = MarkdownTableWriter()
    latex_writer = LatexTableWriter()
    md_writer.headers = ["Task", "Language", "Version", "Metric", "Value", "", "Stderr"]
    latex_writer.headers = ["Task", "Language", "Version", "Metric", "Value", "", "Stderr"]

    values = []
    # XCopa, XWinograd, XStoryCloze
    # tasks_values = [[], [], []]
    tasks_values = [[]]

    for k, dic in sorted(file_json["results"].items()):
        if "_" not in k:
            continue

        version = file_json["versions"][k]
        percent = k == "squad2"

        task = k.split("_")[0]
        lang = k.split("_")[1]
        if task == "xnli":
            task_index = 0
        elif task == "xwinograd":
            task_index = 1
        elif task == "xstorycloze":
            task_index = 2
        for m, v in dic.items():
            # print(m, dic)
            if m.endswith("_stderr,none"):
                continue
            if m == 'alias':
                continue

            m = m.split(",")[0]

            # The eval provide std error, for now, skip those
            # if m + "_stderr,none" in dic:
                # se = dic[m + "_stderr,none"]
                # if percent or m == "ppl":
                #     values.append([task, lang, version, m, "%.2f" % v, "±", "%.2f" % se])
                # else:
                #     values.append(
                #         [task, lang, version, m, "%.2f" % (v * 100), "±", "%.2f" % (se * 100)]
                #     )
            # else:
            if percent or m == "ppl":
                values.append([task, lang, version, m, "%.2f" % v, "", ""])
                tasks_values[task_index].append([task, lang, version, v])
            else:
                try:
                  values.append([task, lang, version, m, "%.2f" % (v * 100), "", ""])
                  tasks_values[task_index].append([task, lang, version, v])
                except:
                  pass
            k = ""
            version = ""
    md_writer.value_matrix = values
    latex_writer.value_matrix = values

    # todo: make latex table look good
    # print(latex_writer.dumps())

    return tasks_values

"""# WandB Logging"""

import pprint
print(pprint.pformat(result))

with wandb.init(
    entity=ENTITY, project=PROJECT, config=wandb_config, name=wandb_runname
) as run:
    # Log Accuracy
    clean_result = make_table(result)[0]
    for task_name, lang_eval, task_version, accuracy in clean_result:
        run.summary[f"{task_name}_acc_{lang_eval}"] = accuracy

    # Log Result
    # columns = ["Eval Dataset", "Result"]
    # data = [["xnli", pprint.pformat(result)]]
    # table = wandb.Table(data=data, columns=columns)
    # run.log({"result": table})

    artifact = wandb.Artifact(name=f"{wandb_runname}-result", type="eval-result")
    artifact.add_file(local_path=f"./{output_result_slimllm}", name=f"{evaluation_dataset}-result.json")
    artifact.save()

