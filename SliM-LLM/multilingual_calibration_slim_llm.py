# -*- coding: utf-8 -*-
"""Multilingual Calibration - Slim LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xJaKsoEojxOt5LwojaVWiCXXVi0ZRLRa
"""



"""# Usual Coding"""
import os
from dotenv import load_dotenv

import torch

import numpy as np
import pandas as pd

from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset

from transformers import AutoTokenizer
from huggingface_hub import create_repo
from huggingface_hub import HfApi

# Argument
import argparse

parser = argparse.ArgumentParser("args_slim-llm")
parser.add_argument("--lang", type=str)
parser.add_argument("--bit", type=int)

# Login

load_dotenv()

if "HF_KEY" not in os.environ:
    raise EnvironmentError("HF_KEY environment variable is not defined. Please set it before running the application.")
if "WANDB_KEY" not in os.environ:
    raise EnvironmentError("WANDB_KEY environment variable is not defined. Please set it before running the application.")

hf_key = os.environ["HF_KEY"]
wandb_key = os.environ["WANDB_KEY"]

from huggingface_hub import login
login(token=hf_key)

"""# Parameter"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'

model_id = "google/gemma-3-1b-pt"

dataset_id = "openlanguagedata/flores_plus"

## Calibration size
NUM_CALIBRATION_SAMPLES=512
MAX_SEQUENCE_LENGTH=2048

"""# Load Model"""

# Commented out IPython magic to ensure Python compatibility.
# %cd SliM-LLM/slim-llm

!python run.py \
 facebook/opt-1.3b wikitext2 8bit --groupsize 128 \
--device "cuda" --save --pack --seed 1234 --nsamples 2

!python run.py \
 "google/gemma-3-1b-pt" wikitext2 4bit --groupsize 128 \
--device "cuda" --save --pack --seed 1234 --nsamples 8

!python run.py \
 "google/gemma-3-1b-pt" flores 8bit --dataset_subset Indonesian --groupsize 128 \
--device "cuda" --save --seed 1234 --nsamples 8

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-pt")
model = AutoModelForCausalLM.from_pretrained("google/gemma-3-1b-pt")

model

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b-it")
model = AutoModelForCausalLM.from_pretrained("google/gemma-2-2b-it")

model

model.config

"""## GPTQ"""

!pip install protobuf==5.29.3
!pip install lm-eval>=0.4.7

import os

import torch
import torch.nn as nn
import torch.nn.functional as F
from lm_eval.models.huggingface import HFLM
from lm_eval import simple_evaluate

import numpy as np
import pickle

from functools import partial

"""# Function"""

def lm_eval_wrapper(model, tokenizer, device: str):
  return HFLM(
    pretrained = model,
    backend = "causal",
    tokenizer = tokenizer,
    trust_remote_code = True,
    device = device,
    dtype = torch.float16,
)

def eval_model(model, device='cpu'):
  return simple_evaluate(
      model=model,
      tasks=["xnli"],
      device=device,
      limit=3,
      num_fewshot=None,  # zero-shot
      apply_chat_template = True,
      gen_kwargs={'temperature': 0},
      predict_only=False,
      batch_size=1,
      random_seed=1234,
  )

"""# Parameter"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device_str = 'cuda' if torch.cuda.is_available() else 'cpu'
batch_size = 1
bit = 4

# Model
model_name = "google/gemma-3-1b-pt"
model_path_gptq = "/content/SliM-LLM/slim-llm/output/google_gemma-3-1b-pt_flores_8bit_128.pt"

result_path_gptq = "/content/drive/MyDrive/Multilingual/Chinese/result_gptq.json"

# Activation
input_store = []
activation_store = {}
hook_store = {}
module_name = "post_attention_layernorm"
layer_ablation = None

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-it")
model = AutoModelForCausalLM.from_pretrained(model_path_gptq)

model = lm_eval_wrapper(model, tokenizer, device_str)

result = eval_model(model, device_str)

result

import pickle
with open(result_path_gptq, 'wb') as file:
    pickle.dump(result, file)

# Script from the lm_eval library
import json
import logging
import os

from pytablewriter import LatexTableWriter, MarkdownTableWriter


logger = logging.getLogger(__name__)


def make_table(result_dict):
    """Generate table of results."""
    md_writer = MarkdownTableWriter()
    latex_writer = LatexTableWriter()
    md_writer.headers = ["Task", "Version", "Metric", "Value", "", "Stderr"]
    latex_writer.headers = ["Task", "Version", "Metric", "Value", "", "Stderr"]

    values = []

    for k, dic in sorted(result_dict["results"].items()):
        version = result_dict["versions"][k]
        percent = k == "squad2"
        for m, v in dic.items():
            if m.endswith("_stderr"):
                continue
            if m == 'alias':
                continue

            if m + "_stderr" in dic:
                se = dic[m + "_stderr"]
                if percent or m == "ppl":
                    values.append([k, version, m, "%.2f" % v, "±", "%.2f" % se])
                else:
                    values.append(
                        [k, version, m, "%.2f" % (v * 100), "±", "%.2f" % (se * 100)]
                    )
            else:
                if percent or m == "ppl":
                    values.append([k, version, m, "%.2f" % v, "", ""])
                else:
                    try:
                      values.append([k, version, m, "%.2f" % (v * 100), "", ""])
                    except:
                      pass
            k = ""
            version = ""
    md_writer.value_matrix = values
    latex_writer.value_matrix = values

    # todo: make latex table look good
    # print(latex_writer.dumps())

    return md_writer.dumps()

print(make_table(result))

