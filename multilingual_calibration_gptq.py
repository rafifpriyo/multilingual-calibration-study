# -*- coding: utf-8 -*-
"""multilingual-calibration-gptq

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/fifrio/multilingual-calibration-gptq-8bit.c86b8d9d-4b56-4f6a-bf90-f2df7774f696.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250930/auto/storage/goog4_request%26X-Goog-Date%3D20250930T092826Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D22958efad3d670fa5a59eba11b891418edaf4bb0eee68bd8aa840b9f813a9a066b2d5144effb914ee9145fd7989f0bad958276c406cbc588d96286f31fb80814bdf6892b38741b4b45c731ba093e4c8721dfc5376d1b03e367160c296f7a57362e2d50fb6370a057f2ed68b2ca1453e8b984ff19ff8ef1f1b0306b19a1016f570b550912f0b8f264ae163be02eb7e81190abfa7c0e5888234373bfa85d013a68f25d720f75491bf47317e6e4a62b6c8969c9e767d3dddb2f65d9482906c89d464c8696e934a5bfd7a71d7495ac07aec2ed50109444abf54e06dab5d155941ca66dec554a3adb01bda6a1cc3e7591c5c5119a6ca682ee8b57f5979fbfa7d3b6e9
"""

import os
from dotenv import load_dotenv

import torch

import numpy as np
import pandas as pd

from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
from gptqmodel import GPTQModel, QuantizeConfig

from transformers import AutoTokenizer
from huggingface_hub import create_repo
from huggingface_hub import HfApi

# Argument
import argparse

parser = argparse.ArgumentParser("args_gptq")
parser.add_argument("--lang", type=str)
parser.add_argument("--bit", type=int)

"""# Parameter"""

load_dotenv()

if "HF_KEY" not in os.environ:
    raise EnvironmentError("HF_KEY environment variable is not defined. Please set it before running the application.")
if "WANDB_KEY" not in os.environ:
    raise EnvironmentError("WANDB_KEY environment variable is not defined. Please set it before running the application.")

hf_key = os.environ["HF_KEY"]
wandb_key = os.environ["WANDB_KEY"]

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Model
model_id = "google/gemma-3-1b-pt"

# Calibration Dataset
# lang_lst = ["Estonian", "Haitian", "Indonesian", "Italian",
#              "Quechua", "Swahili", "Tamil", "Thai",
#              "Turkish", "Vietnamese", "Chinese"]
# ISO_3_lst = ["ekk_Latn", "hat_Latn", "ind_Latn", "ita_Latn",
#              "quy_Latn", "swh_Latn", "tam_Taml", "tha_Thai",
#              "tur_Latn", "vie_Latn", "wuu_Hans"]
# ISO_2_lst = ["et", "ht", "id", "it",
#              "qu", "sw", "ta", "th",
#              "tr", "vi", "zh"]

lang_lst = ["Indonesian",
             "Tamil",
             "Chinese"]
ISO_3_lst = ["ind_Latn",
             "tam_Taml",
             "wuu_Hans"]
ISO_2_lst = ["id",
             "ta",
             "zh"]

args = parser.parse_args()
lang = args.lang
bit = args.bit
ISO_3 = ISO_3_lst[lang_lst.index(lang)]

dataset_split = "dev"

dataset_id = "openlanguagedata/flores_plus"

# Quantization
quantization_technique = "gptq"
bit_lst = [4, 8]

output_path_gptq = f"./{model_id.split("/")[-1]}_{quantization_technique}_{{bit}}bit_{{lang}}"

## Calibration size
SYMMETRY = False
GROUP_SIZE=128
NUM_CALIBRATION_SAMPLES=512
MAX_SEQUENCE_LENGTH=2048

# huggingface
output_huggingface_gptq = f"fifrio/{model_id.split("/")[-1]}-{quantization_technique}_{{bit}}bit_{{lang}}"

"""# Looping"""

#for bit in bit_lst:
for i in range(1):
    for j in range(1):
    #for lang, ISO_3 in zip(lang_lst, ISO_3_lst):
        """## Calibration data

        > https://huggingface.co/datasets/cambridgeltl/xcopa
        >
        > https://huggingface.co/datasets/Muennighoff/xwinograd
        >
        > https://huggingface.co/datasets/juletxara/xstory_cloze
        """

        tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_key)
        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device, token=hf_key)

        # Load dataset.
        ds = load_dataset(dataset_id, ISO_3, split="dev", token=hf_key)
        ds = ds.shuffle(seed=42)

        def chat_template(text):
            return [
                {"role": "user", "content": text},
        ]

        # Preprocess the data into the format the model is trained with.
        def preprocess(example):
            # return {"text": tokenizer.apply_chat_template(chat_template(example["text"]), tokenize=False,)}
            return {"text": example["text"]}
        ds = ds.map(preprocess)

        """## GPTQ"""

        quant_config = QuantizeConfig(bits=bit, group_size=GROUP_SIZE, sym=SYMMETRY, lm_head=False, device=device)
        model = GPTQModel.load(model_id, quant_config, token=hf_key)

        model.quantize(ds['text'][:NUM_CALIBRATION_SAMPLES], batch_size=1)
        model.save(output_path_gptq.format(lang=lang))

        """# Upload to Huggingface"""

        create_repo(output_huggingface_gptq.format(bit=bit, lang=lang), repo_type="model", token=hf_key)

        api = HfApi(token=hf_key)

        api.upload_folder(
            folder_path=output_path_gptq.format(bit=bit, lang=lang),
            repo_id=output_huggingface_gptq.format(bit=bit, lang=lang),
            repo_type="model",
        )

