# -*- coding: utf-8 -*-
"""multilingual-evaluation-gptq

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/fifrio/multilingual-evaluation-gptq-wandb-typo.9ae5501b-7104-479e-a72d-692599c55754.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250930/auto/storage/goog4_request%26X-Goog-Date%3D20250930T120535Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D759c1b4d5eb61df62a4fdc301f919cb9c2bf6bbe17efdf641d451654fdd9f4f8d2f11cb522da94f40fa5e21fa83dcdab283028f02ef92d346123a6beef9d28812c7ac5da08fff93d85b71d8cf310fc05904fad956337aafb847c307e6686a7ef3df520bb33f59a888b103db5370971627975a11ec90cc8216da0d2588336560b3409913ec1237fc05ce01e6dcb9fc0de9c93dbade6de48d5088685c50382aae7219f58e60242d5278c9b1b648cdc0d8e733178642025f238c9967a4bfa2e6d2856e0bee73471e96cb58cd7e474a54b6dbe6511636074842a25b450159683dffda949de4aae90febec78d3d519b31555e17493a41f20b871687149d5a88a79aba
"""

import os
from dotenv import load_dotenv

import torch
from lm_eval.models.huggingface import HFLM
from lm_eval import simple_evaluate

import numpy as np
import pickle
import pprint

from gptqmodel import GPTQModel
import wandb

from pytablewriter import LatexTableWriter, MarkdownTableWriter

"""# Parameter"""

load_dotenv()

if "HF_KEY" not in os.environ:
    raise EnvironmentError("HF_KEY environment variable is not defined. Please set it before running the application.")
if "WANDB_KEY" not in os.environ:
    raise EnvironmentError("WANDB_KEY environment variable is not defined. Please set it before running the application.")

hf_key = os.environ["HF_KEY"]
wandb_key = os.environ["WANDB_KEY"]

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device_str = 'cuda' if torch.cuda.is_available() else 'cpu'
batch_size = 1
quantization_technique = "gptq"
bit_lst = [4, 8]

# Model
model_id = "google/gemma-3-1b-it"
model_path_gptq = f"./{model_id.split("/")[-1]}_{quantization_technique}_{{bit}}bit_{{lang}}"

# Evaluation
num_shot = 3

# Calibration Dataset
lang_lst = ["Estonian", "Haitian", "Indonesian", "Italian",
             "Quechua", "Swahili", "Tamil", "Thai",
             "Turkish", "Vietnamese", "Chinese"]
ISO_3_lst = ["ekk_Latn", "hat_Latn", "ind_Latn", "ita_Latn",
             "quy_Latn", "swh_Latn", "tam_Taml", "tha_Thai",
             "tur_Latn", "vie_Latn", "wuu_Hans"]
ISO_2_lst = ["et", "ht", "id", "it",
             "qu", "sw", "ta", "th",
             "tr", "vi", "zh"]

result_path_gptq = f"./xcopa_{num_shot}shot_{quantization_technique}_{{bit}}bit_{{lang}}.json"

# WandB Logging
output_huggingface_gptq = f"fifrio/{model_id}-{quantization_technique}-{{bit}}bit-calibration-{{lang}}"

wandb_runname = f"{model_id}-{quantization_technique}-{{bit}}bit-{{lang}}-1"

"""# Function"""

def lm_eval_wrapper(model, tokenizer, device: str):
  return HFLM(
    pretrained = model,
    backend = "causal",
    tokenizer = tokenizer,
    trust_remote_code = True,
    device = device,
    dtype = torch.float16,
)

def eval_model(model, device='cpu'):
  return simple_evaluate(
      model=model,
      tasks=["xcopa",],
            # "xwinograd",
            #  "xstorycloze"],
      device=device,
      num_fewshot=num_shot,
      apply_chat_template = True,
      gen_kwargs={'temperature': 0},
      predict_only=False,
      batch_size=1,
      random_seed=1234,
  )

"""# Looping"""
for bit in bit_lst:
    for lang in lang_lst:
        wandb_config = {
            'base_model': model_id,
            'quantization_technique': quantization_technique,
            'bit_width': "4-bit",
            "group_size": 128,
            "calibration_language": lang,
            'num_calibration_samples': 512,
            'max_sequence_length': 2048,
            'symmetry': False,
            'output_huggingface': output_huggingface_gptq.format(bit=bit, lang=lang),
            'num_shot': num_shot,
        }

        """# GPTQ"""

        model = GPTQModel.load(model_path_gptq.format(bit=bit, lang=lang))
        # res = model.generate("Who are you?")[0]
        # model.tokenizer.decode(res)

        model = lm_eval_wrapper(model, model.tokenizer, device)

        result_gptq = eval_model(model, device)

        with open(result_path_gptq.format(bit=bit, lang=lang), 'wb') as file:
            pickle.dump(result_gptq, file)

        def make_table(file_json):
        # Script from the lm_eval library
            """Generate table of results."""
            md_writer = MarkdownTableWriter()
            latex_writer = LatexTableWriter()
            md_writer.headers = ["Task", "Language", "Version", "Metric", "Value", "", "Stderr"]
            latex_writer.headers = ["Task", "Language", "Version", "Metric", "Value", "", "Stderr"]

            values = []
            # XCopa, XWinograd, XStoryCloze
            # tasks_values = [[], [], []]
            tasks_values = [[]]

            for k, dic in sorted(file_json["results"].items()):
                if "_" not in k:
                    continue

                version = file_json["versions"][k]
                percent = k == "squad2"

                task = k.split("_")[0]
                lang = k.split("_")[1]
                if task == "xcopa":
                    task_index = 0
                elif task == "xwinograd":
                    task_index = 1
                elif task == "xstorycloze":
                    task_index = 2
                for m, v in dic.items():
                    # print(m, dic)
                    if m.endswith("_stderr,none"):
                        continue
                    if m == 'alias':
                        continue

                    m = m.split(",")[0]

                    # The eval provide std error, for now, skip those
                    # if m + "_stderr,none" in dic:
                        # se = dic[m + "_stderr,none"]
                        # if percent or m == "ppl":
                        #     values.append([task, lang, version, m, "%.2f" % v, "±", "%.2f" % se])
                        # else:
                        #     values.append(
                        #         [task, lang, version, m, "%.2f" % (v * 100), "±", "%.2f" % (se * 100)]
                        #     )
                    # else:
                    if percent or m == "ppl":
                        values.append([task, lang, version, m, "%.2f" % v, "", ""])
                        tasks_values[task_index].append([task, lang, version, v])
                    else:
                        try:
                            values.append([task, lang, version, m, "%.2f" % (v * 100), "", ""])
                            tasks_values[task_index].append([task, lang, version, v])
                        except:
                            pass
                    k = ""
                    version = ""
            md_writer.value_matrix = values
            latex_writer.value_matrix = values

            # todo: make latex table look good
            # print(latex_writer.dumps())

            return tasks_values

        """# Log to wandb"""

        wandb.login(key=wandb_key)

        ENTITY = wandb.apis.PublicApi().default_entity
        PROJECT = "calibration-on-quantized-multilingual"

        """> Log Model and Summary Result also the file"""

        with wandb.init(
            entity=ENTITY, project=PROJECT, config=wandb_config, name=wandb_runname.format(bit=bit, lang=lang)
        ) as run:
            # Log Accuracy
            clean_result = make_table(result_gptq)[0]
            for task_name, lang_eval, task_version, accuracy in clean_result:
                run.summary[f"{task_name}_acc_{lang_eval}"] = accuracy

            # Log Result
            columns = ["Eval Dataset", "Result"]
            data = [["xcopa", pprint.pformat(result_gptq)]]
            table = wandb.Table(data=data, columns=columns)
            run.log({"result": table})