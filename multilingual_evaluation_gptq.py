# -*- coding: utf-8 -*-
"""multilingual-evaluation-gptq

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/fifrio/multilingual-evaluation-gptq-wandb-typo.9ae5501b-7104-479e-a72d-692599c55754.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250930/auto/storage/goog4_request%26X-Goog-Date%3D20250930T120535Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D759c1b4d5eb61df62a4fdc301f919cb9c2bf6bbe17efdf641d451654fdd9f4f8d2f11cb522da94f40fa5e21fa83dcdab283028f02ef92d346123a6beef9d28812c7ac5da08fff93d85b71d8cf310fc05904fad956337aafb847c307e6686a7ef3df520bb33f59a888b103db5370971627975a11ec90cc8216da0d2588336560b3409913ec1237fc05ce01e6dcb9fc0de9c93dbade6de48d5088685c50382aae7219f58e60242d5278c9b1b648cdc0d8e733178642025f238c9967a4bfa2e6d2856e0bee73471e96cb58cd7e474a54b6dbe6511636074842a25b450159683dffda949de4aae90febec78d3d519b31555e17493a41f20b871687149d5a88a79aba
"""

import os
from dotenv import load_dotenv

import torch
# from lm_eval.models.huggingface import HFLM
from lm_eval.models.vllm_causallms import VLLM
from lm_eval import simple_evaluate

import numpy as np
import pickle
import pprint

from gptqmodel import GPTQModel
import wandb

from pytablewriter import LatexTableWriter, MarkdownTableWriter

"""# Parameter"""

load_dotenv()

if "HF_KEY" not in os.environ:
    raise EnvironmentError("HF_KEY environment variable is not defined. Please set it before running the application.")
if "WANDB_KEY" not in os.environ:
    raise EnvironmentError("WANDB_KEY environment variable is not defined. Please set it before running the application.")

hf_key = os.environ["HF_KEY"]
wandb_key = os.environ["WANDB_KEY"]

from huggingface_hub import login
login(token=hf_key)

# Argument
import argparse

parser = argparse.ArgumentParser("args_gptq")
parser.add_argument("--lang", type=str)
parser.add_argument("--bit", type=int)
args = parser.parse_args()
lang = args.lang
bit = args.bit
print(f"lang {lang} bit {bit}")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device_str = 'cuda' if torch.cuda.is_available() else 'cpu'
batch_size = 4
bit_lst = [4, 8]

# Model
model_id = "Qwen/Qwen3-1.7B"

# Evaluation
evaluation_dataset = "xnli"
num_shot = 3
apply_chat_template = True
enable_thinking = False

# Quantization Config
quantization_technique = "gptq"
granularity = "group"
group_size = 128
num_calibration_samples = 512
max_sequence_length = 2048
symmetry = False

# Calibration Dataset
# lang_lst = ["Estonian", "Haitian", "Indonesian", "Italian",
#              "Quechua", "Swahili", "Tamil", "Thai",
#              "Turkish", "Vietnamese", "Chinese"]
# ISO_3_lst = ["ekk_Latn", "hat_Latn", "ind_Latn", "ita_Latn",
#              "quy_Latn", "swh_Latn", "tam_Taml", "tha_Thai",
#              "tur_Latn", "vie_Latn", "wuu_Hans"]
# ISO_2_lst = ["et", "ht", "id", "it",
#              "qu", "sw", "ta", "th",
#              "tr", "vi", "zh"]

lang_lst = ["English",
            "Indonesian",
             "Tamil",
             "Swahili",
             "Chinese"]
ISO_3_lst = ["eng_Latn",
             "ind_Latn",
             "tam_Taml",
             "swh_Latn",
             "wuu_Hans"]
ISO_2_lst = ["en",
             "id",
             "ta",
             "sw",
             "zh"]

model_path_gptq = f"./{model_id.split('/')[-1]}_{quantization_technique}_{{bit}}bit_{{lang}}"
result_path_gptq = f"./{evaluation_dataset}_{num_shot}shot_{quantization_technique}_{{bit}}bit_{{lang}}.json"

# WandB Logging
output_huggingface_gptq = f"fifrio/{model_id.split('/')[-1]}-{quantization_technique}-{{bit}}bit-calibration-{{lang}}"

wandb_runname = f"{model_id.split('/')[-1]}-{quantization_technique}-{{bit}}bit-{{lang}}-{evaluation_dataset}"

"""# Function"""

def lm_eval_wrapper(model, tokenizer, device: str):
#   return HFLM(
  return VLLM(
    pretrained = model,
    tokenizer = tokenizer,
    trust_remote_code = True,
    device = device,
    dtype = "float16",
    # gptqmodel=True,
    batch_size=batch_size,
    enable_thinking=enable_thinking,
)

def eval_model(model, device='cpu'):
  return simple_evaluate(
      model=model,
      tasks=[evaluation_dataset],
            # "xwinograd",
            #  "xstorycloze"],
      device=device,
      num_fewshot=num_shot,
      apply_chat_template = apply_chat_template,
      # gen_kwargs={'temperature': 0},
      predict_only=False,
      log_samples=True,
      batch_size=batch_size,
      random_seed=1234,
  )

"""# Looping"""
# for bit in bit_lst:
for i in range(1):
    for j in range(1):
    # for lang in lang_lst:
        wandb_config = {
            'base_model': model_id,
            'quantization_technique': quantization_technique,
            'calibration_language': lang,
            'bit_width': f"{bit}-bit",
            "granularity": granularity,
            "group_size": group_size,
            'num_calibration_samples': num_calibration_samples,
            'max_sequence_length': max_sequence_length,
            'symmetry': symmetry,
            'output_huggingface': output_huggingface_gptq.format(bit=bit, lang=lang),
            'evaluation_dataset': evaluation_dataset,
            'num_shot': num_shot,
            'apply_chat_template': apply_chat_template,
            'enable_thinking': enable_thinking,
        }

        """# GPTQ"""

        model = GPTQModel.load(model_path_gptq.format(bit=bit, lang=lang), device_map='auto', device=device)
        # res = model.generate("Who are you?")[0]
        # model.tokenizer.decode(res)

        model = lm_eval_wrapper(model, model.tokenizer, device_str)

        result_gptq = eval_model(model, device_str)

        with open(result_path_gptq.format(bit=bit, lang=lang), 'wb') as file:
            pickle.dump(result_gptq, file)

        def make_table(file_json):
        # Script from the lm_eval library
            """Generate table of results."""
            md_writer = MarkdownTableWriter()
            latex_writer = LatexTableWriter()
            md_writer.headers = ["Task", "Language", "Version", "Metric", "Value", "", "Stderr"]
            latex_writer.headers = ["Task", "Language", "Version", "Metric", "Value", "", "Stderr"]

            values = []
            # XCopa, XWinograd, XStoryCloze
            # tasks_values = [[], [], []]
            tasks_values = [[]]

            for k, dic in sorted(file_json["results"].items()):
                if "_" not in k:
                    continue

                version = file_json["versions"][k]
                percent = k == "squad2"

                task = k.split("_")[0]
                lang = k.split("_")[1]
                if task == "xnli":
                    task_index = 0
                elif task == "xwinograd":
                    task_index = 1
                elif task == "xstorycloze":
                    task_index = 2
                for m, v in dic.items():
                    # print(m, dic)
                    if m.endswith("_stderr,none"):
                        continue
                    if m == 'alias':
                        continue

                    m = m.split(",")[0]

                    # The eval provide std error, for now, skip those
                    # if m + "_stderr,none" in dic:
                        # se = dic[m + "_stderr,none"]
                        # if percent or m == "ppl":
                        #     values.append([task, lang, version, m, "%.2f" % v, "±", "%.2f" % se])
                        # else:
                        #     values.append(
                        #         [task, lang, version, m, "%.2f" % (v * 100), "±", "%.2f" % (se * 100)]
                        #     )
                    # else:
                    if percent or m == "ppl":
                        values.append([task, lang, version, m, "%.2f" % v, "", ""])
                        tasks_values[task_index].append([task, lang, version, v])
                    else:
                        try:
                            values.append([task, lang, version, m, "%.2f" % (v * 100), "", ""])
                            tasks_values[task_index].append([task, lang, version, v])
                        except:
                            pass
                    k = ""
                    version = ""
            md_writer.value_matrix = values
            latex_writer.value_matrix = values

            # todo: make latex table look good
            # print(latex_writer.dumps())

            return tasks_values

        """# Log to wandb"""

        wandb.login(key=wandb_key)

        ENTITY = wandb.apis.PublicApi().default_entity
        PROJECT = "calibration-on-quantized-multilingual"

        """> Log Model and Summary Result also the file"""

        with wandb.init(
            entity=ENTITY, project=PROJECT, config=wandb_config, name=wandb_runname.format(bit=bit, lang=lang)
        ) as run:
            # Log Accuracy
            clean_result = make_table(result_gptq)[0]
            for task_name, lang_eval, task_version, accuracy in clean_result:
                run.summary[f"{task_name}_acc_{lang_eval}"] = accuracy

            # Log Result
            # columns = ["Eval Dataset", "Result"]
            # data = [["xnli", pprint.pformat(result_gptq)]]
            # table = wandb.Table(data=data, columns=columns)
            # run.log({"result": table})

            artifact = wandb.Artifact(name=f"{wandb_runname.format(bit=bit, lang=lang)}-result", type="eval-result")
            artifact.add_file(local_path=f"./{result_path_gptq.format(bit=bit, lang=lang)}", name=f"{evaluation_dataset}-result.json")
            artifact.save()